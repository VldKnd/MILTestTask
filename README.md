#### Структура:
```
.
├── cfg                   
│   ├── 64_200.json     ### Файл с конфигурацией нейронной сети и DataLoader, которые я использовал для обучения ResNet
│   └── example.json    ### Пример возможного файла с конфигурацией
├── chkp                ### Сохраненные веса для моделей  
│   ├── 64_200          ### Веса для не квантированного ResNet20, точность на тест 92%
│   ├── 64_200_int2     ### Веса для квантированного ResNet20 в int2
│   ├── 64_200_int4     ### Веса для квантированного ResNet20 в int4
│   └── 64_200_int8     ### Веса для квантированного ResNet20 в int8 
├── src
│   ├── model.py        ### Файл с имплементацией ResNet блока и архитектуры               
│   ├── qmodel.py       ### Файл с имплементацией квантированных аналогов блоков из PyTorch        
│   ├── train.py        ### Функции для тренировки и валидации нейронной сети  
│   ├── utils.py        ### Всякие полезные функции
│   └── __init__.py
├── PTQ.ipynb           ### Короткий нотбук с реализацией квантированной ResNet20 в int2, int4, int8. В int8 с совмещением Conv2d и Batch Norm.
│                       ### Так же тестирование моей имплементации квантизирования и основные результаты
├── train_resnet.py     ### Файл с тренировкой ResNet20
├── requirements.txt
└── README.md
```
1.2.3.4. Что бы скачать CIFAR10, можно в конфигурации поставить download:true. Тогда при запуске train_resnet.py скачает его в папку data.
train_resnet можно использовать для тренировки ResNet20. Я оставил возможность поменять часть пораметров тренировки. Такие как размер батча и количесвто эпох тренировки. Остальные параметры зафиксированны, их можно посмотреть в train_resnet.py. Я использовал [архитектуру](https://www.researchgate.net/figure/ResNet-20-architecture_fig3_351046093) с нормализацией по батчу после каждой конволюции.

Пример конфигурации:
```json
{
    "_comment_json": "Поле для комментариев к переменным",
    
    "cfg":{
        "_comment_chekpoint_path": "Куда сохранять веса модели",
        "checkpoint_path":"./chkp/64_200"
    },

    "cfg_CIFAR":{
        "_comment_root": "Куда загружать или откуда выгружать CIFAR10",
        "root":"./data",
        "_comment_download": "Загружать или не загружать данные",
        "download":true
    },
    
    "_comment_cfg_dataloader_train":"Конфигурация для trainloader",
    "cfg_dataloader_train":{
        "batch_size":64,
        "shuffle":true,
        "num_workers":2,
        "pin_memory":true
    },

    "_comment_cfg_dataloader_test":"Конфигурация для testloader",
    "cfg_dataloader_test":{
        "batch_size":1024,
        "shuffle":false,
        "num_workers":2,
        "pin_memory":true
    },

    "_comment_cfg_train":"Конфигурация для цикла тренировки",
    "cfg_train":{
        "n_epoches":200
    }
}
```
Пример использовния train_resnet.py
```
python train_resnet.py cfg/64_200.json
```
5. Я реализовал готовое решение квантизации в PTQ.ipynb. Я выбрал самый простой метод сбора статистик (MinMax) его конечно можно улучшить выбрав более подходящий модуль, который представлен в PyTorch, например Гистограммы для сбора статистик. 

### Attention

> Квантовать к 16 и 8 битам. Квантовать уже обученную модель, которая была получена на шаге 4.
> 
Вы попросили проквантовать сеть к 16 и 8 битам, но [торч пока что поддерживает только int8](https://discuss.pytorch.org/t/expending-pytorch-with-lower-than-8-bit-quantization/80343). Поэтому результаты получились предсказуеммыми только для int8. Более того, для квантизации весов торч поддерживает только uint8. Не смотря на это, можно ограничить промежуток используемых для квантизации значений, тем самым "симулировать" int4/int2 в методах представленных в torch. Однако, все они так же будут хранится в int8/uint8, из чего следует, что размер модели квантированной в int4 или int2 не будет отличаться от модели в int8 (не смотря на то, что должен) и инференс тайм будет тем же. Квантизация ниже int8 просто сильно ухудшит качество предсказания. См результаты ниже.

Так же, для сравнения я добавил эксперименты с моделью, в которой я совместил Conv2d, BatchNorm2d, ReLU, которые идут подряд.

6. Я реализовал сразу квантизацию сети с совмещенными Conv2d, BatchNorm2d, ReLU (код можно довольно быстро расширить и на случай без совмещения слоев). Весь код с реализацией слоев находится в src/qmodel.py . Логика, которую я реализовал копирует модель и пропускает несколько батчей, сохраняя при этом активации после всех выходов, что бы после их точно квантизировать. После этого, вызывая compile у каждого квантированного модуля, код чистит не нужные веса и сохраняет получившуюся квантированную модель. 


### Attention

> Квантовать к 16, 8, 4 и 2 битам. Квантовать уже обученную модель, которая была получена на шаге 4.

Ситуация такая же, как и в пункте 5.

7. Здесь я представлю получившиеся результаты, они все взяты из PTQ.ipynb .

**Results** | Original | PyTorch int2 | PyTorch int4 | PyTorch int8 | Fused PyTorch int8 | My Model int8
------ | ------ | ------ | ------ | ------ | ------ | ------ 
Accuracy | 92.1% | 11.1% | 29.6% | 88.1% | 91.3% | 91.4% 
Inference Time | 39.5s ± 0.8s | 22.9s ± 0.3s | 22.9s ± 0.3s | 22.9s ± 0.3s | 15.6s ± 0.1s | 16.8s ± 0.3s 
Size | 1.596 MB | 0.485 MB | 0.485 MB | 0.485 MB | 0.430 MB | 0.425 MB  


[Vladimir Kondratyev](https://github.com/VldKnd)

---
**Шаг 1.** Ознакомиться с понятием Quantization.  

На данном шаге предлагается разобраться с понятием Quantization и со стандартными техниками, как dynamic, static, post training и quantization aware quantization.

**Шаг 2.** Скачать датасет CIFAR10.

**Шаг 3.** Реализовать архитектуру ResNet20.

**Шаг 4.** Обучить ResNet20.  

**Шаг 5.** Применить готовые решения для post training quantization (далее PTQ).  

> Квантовать к 16 и 8 битам. Квантовать уже обученную модель, которая была получена на шаге 4.

**Шаг 6.** Реализовать PTQ.

> Квантовать к 16, 8, 4 и 2 битам. Квантовать уже обученную модель, которая была получена на шаге 4.

**Шаг 7.** Сравнение результатов

---
